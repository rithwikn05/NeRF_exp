{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f8530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tiny_nerf_data.npz...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "#NeRF model architecture\n",
    "\n",
    "'''\n",
    "Components to the model:\n",
    "1. Position encoding (finished)\n",
    "2. Linear layer with ReLU activation function\n",
    "3. Skip connection which adds the position encoding back in\n",
    "4. Towards the end, the model splits into two sets of weights: One which maps to the sigma value (density), and one which which goes to rest of layers. This layer doesn't have an activation function\n",
    "5. View encoding\n",
    "6. At the last layer, in order to map to RGB, we use a sigmoid activation function\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interactive, widgets\n",
    "\n",
    "# if not os.path.exists('tiny_nerf_data.npz'):\n",
    "#     !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "if not os.path.exists('tiny_nerf_data.npz'):\n",
    "    print(\"Downloading tiny_nerf_data.npz...\")\n",
    "    url = \"http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\"\n",
    "    response = requests.get(url)\n",
    "    with open('tiny_nerf_data.npz', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2404702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def encoding(x, L=10):\n",
    "    res = []\n",
    "    for i in range(L):\n",
    "        for fn in [torch.sin, torch.cos]:\n",
    "            res.append(2 ** (i * i * torch.pi * x))\n",
    "    return torch.cat(res, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176e1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, D = 8, W = 256, input_ch = 60, input_ch_views = 24, skips = [4]):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.W = W\n",
    "        self.input_ch = input_ch\n",
    "        self.input_ch_views = input_ch_views\n",
    "        self.skips = skips\n",
    "\n",
    "        self.pts_linears = nn.ModuleList(\n",
    "            [nn.Linear(input_ch, W)] + \n",
    "            [nn.Linear(W, W) if i not in skips else nn.Linear(W + input_ch, W) for i in range(D-1)]\n",
    "        )\n",
    "\n",
    "        #The split\n",
    "        self.sigma_linear = nn.Linear(W, 1) #going from 256 to 1\n",
    "\n",
    "        #Feature Vector\n",
    "        self.feature_linear = nn.Linear(W, W)   #Intermediate layers goes from 256 to 256\n",
    "\n",
    "        #View encoding\n",
    "        self.view_linear = nn.Linear(input_ch_views + W, W // 2)    #goes from viewing encoding + 256 to 128 \n",
    "\n",
    "        #RGB\n",
    "        self.rgb_linear = nn.Linear(W//2, 3)   #Going from 128 to 3\n",
    "    \n",
    "    def forward(self, x, view_dirs):\n",
    "        h = x\n",
    "        for i, layer in enumerate(self.pts_linears):\n",
    "            if i in self.skips:\n",
    "                h = torch.cat([h, x], dim = -1)\n",
    "            h = self.pts_linears[i](h)\n",
    "            h = torch.relu(h)\n",
    "        \n",
    "        sigma = torch.relu(self.sigma_linear(h))\n",
    "        feature_vector = self.feature_linear(h)\n",
    "        color_input = torch.cat([feature_vector, view_dirs], dim=-1)\n",
    "        h_final = self.view_linear(color_input)\n",
    "        h_final = torch.relu(h_final)\n",
    "\n",
    "        rgb = torch.sigmoid(self.rgb_linear(h_final))\n",
    "        return rgb, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22a8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(H, W, focal, c2w):\n",
    "  \"\"\"\n",
    "  Generate rays for a given camera configuration.\n",
    "\n",
    "  Args:\n",
    "    H: Image height.\n",
    "    W: Image width.\n",
    "    focal: Focal length.\n",
    "    c2w: Camera-to-world transformation matrix (4x4).\n",
    "\n",
    "  Returns:\n",
    "    rays_o: Ray origins (H*W, 3).\n",
    "    rays_d: Ray directions (H*W, 3).\n",
    "  \"\"\"\n",
    "  device = c2w.device  # Get the device of c2w\n",
    "  focal = torch.from_numpy(focal).to(device)\n",
    "  # print(type(H), type(W), type(focal), type(c2w))\n",
    "\n",
    "  i, j = torch.meshgrid(\n",
    "      torch.arange(W, dtype=torch.float32, device=device),\n",
    "      torch.arange(H, dtype=torch.float32, device=device),\n",
    "      indexing='xy'\n",
    "  )\n",
    "  dirs = torch.stack(\n",
    "      [(i - W * .5) / focal, -(j - H * .5) / focal, -torch.ones_like(i, device = device)], -1\n",
    "  )\n",
    "\n",
    "  rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], -1)\n",
    "  rays_d = rays_d.view(-1, 3)\n",
    "  rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
    "\n",
    "  return rays_o, rays_d\n",
    "\n",
    "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, device, rand=False, embed_fn=None, chunk=1024*4):\n",
    "    def batchify(fn, chunk):\n",
    "        return lambda inputs: torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "\n",
    "    # Sampling\n",
    "    z_vals = torch.linspace(near, far, steps=N_samples, device=device)\n",
    "\n",
    "    if rand:\n",
    "        z_vals += torch.rand(*z_vals.shape[:-1], N_samples, device=rays_o.device) * (far - near) / N_samples\n",
    "\n",
    "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "\n",
    "    # Normalize view directions\n",
    "    view_dirs = rays_d / torch.norm(rays_d, dim=-1, keepdim=True)\n",
    "    view_dirs = view_dirs[..., None, :].expand(pts.shape)\n",
    "\n",
    "    input_pts = torch.cat((pts, view_dirs), dim=-1)\n",
    "    raw = batchify(network_fn, chunk)(input_pts)\n",
    "\n",
    "    # Apply activations here instead of in network\n",
    "    sigma_a = raw[...,0]  # Shape: [batch, N_samples]\n",
    "    rgb = raw[...,1:]    # Shape: [batch, N_samples, 3]\n",
    "\n",
    "    # Improved volume rendering\n",
    "    dists = z_vals[..., 1:] - z_vals[..., :-1]  # Shape: [batch, N_samples-1]\n",
    "    dists = torch.cat([dists, torch.tensor([1e10], device=device)], -1)\n",
    "\n",
    "    # No need to manually expand dists as broadcasting will handle it\n",
    "    alpha = 1. - torch.exp(-sigma_a * dists)  # Shape: [batch, N_samples]\n",
    "    alpha = alpha.unsqueeze(-1)  # Shape: [batch, N_samples, 1]\n",
    "\n",
    "    # Computing transmittance\n",
    "    ones_shape = (alpha.shape[0], 1, 1)\n",
    "    T = torch.cumprod(\n",
    "        torch.cat([\n",
    "            torch.ones(ones_shape, device=device),\n",
    "            1. - alpha + 1e-10\n",
    "        ], dim=1),\n",
    "        dim=1\n",
    "    )[:, :-1]  # Shape: [batch, N_samples, 1]\n",
    "\n",
    "    weights = alpha * T  # Shape: [batch, N_samples, 1]\n",
    "\n",
    "    # Compute final colors and depths\n",
    "    rgb_map = torch.sum(weights * rgb, dim=1)  # Sum along sample dimension\n",
    "    depth_map = torch.sum(weights.squeeze(-1) * z_vals, dim=-1)  # Shape: [batch]\n",
    "    acc_map = torch.sum(weights.squeeze(-1), dim=-1)  # Shape: [batch]\n",
    "\n",
    "    return rgb_map, depth_map, acc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4acc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(images,poses,H,W,focal,testpose,testimg,device):\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = NeRF().to(device)\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "    n_iter = 1000\n",
    "    n_samples = 64\n",
    "    i_plot = 50\n",
    "    psnrs = []\n",
    "    iternums = []\n",
    "    t = time.time()\n",
    "\n",
    "    # Convert data to tensors and move to device ONCE\n",
    "    images_tensor = torch.from_numpy(images).float().to(device)\n",
    "    poses_tensor = torch.from_numpy(poses).float().to(device)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        img_i = np.random.randint(images.shape[0])\n",
    "\n",
    "        target = images_tensor[img_i]  # Use the corresponding image\n",
    "        pose = poses_tensor[img_i]     # Use the corresponding pose\n",
    "\n",
    "        rays_o, rays_d = get_rays(H, W, focal, pose)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=n_samples, device=device, rand=True)\n",
    "\n",
    "        rgb = rgb.reshape(H,W,3)\n",
    "\n",
    "        loss = criterion(rgb, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % i_plot == 0:\n",
    "            print(f'Iteration: {i}, Loss: {loss.item():.6f}, Time: {(time.time() - t) / i_plot:.2f} secs per iter')\n",
    "            t = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                rays_o, rays_d = get_rays(H, W, focal, testpose)\n",
    "                rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6.,\n",
    "                                           N_samples=n_samples, device=device)\n",
    "                rgb = rgb.reshape(H, W, 3)\n",
    "                loss = criterion(rgb, testimg)\n",
    "                psnr = -10. * torch.log10(loss)\n",
    "\n",
    "                psnrs.append(psnr.item())\n",
    "                iternums.append(i)\n",
    "\n",
    "                plt.figure(figsize=(10,4))\n",
    "                plt.subplot(121)\n",
    "                plt.imshow(rgb.cpu().detach())\n",
    "                plt.title(f'Iteration: {i}')\n",
    "                plt.subplot(122)\n",
    "                plt.plot(iternums, psnrs)\n",
    "                plt.title('PSNR')\n",
    "                plt.show()\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
